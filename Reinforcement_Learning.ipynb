{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Reinforcement Learning"
      ],
      "metadata": {
        "id": "3BFpkw2BpZn1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$s_t$ = state at time $t$\n",
        "\n",
        "$a_t$ = action taken at time $t$\n",
        "\n",
        "$r_{t+1}$ = reward after action is taken\n",
        "\n",
        "$G_t = r_{t+1}+r_{t+2}+r_{t+3}+\\cdot\\cdot\\cdot = r_{t+1} + G_{t+1} = r_{t+1} + \\gamma Q(s_{t+1},a_{t+1})$\n",
        "\n",
        "$Q_{new}(s_t,a_t)= Q(s_t,a_t)+\\alpha[r(s_t,a_t)+\\gamma Q(s_{t+1},a_{t+1})-Q(s_t,a_t)]$"
      ],
      "metadata": {
        "id": "hCkDICr6pn3J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Bellman Equation for State Value Function\n",
        "\n",
        "$V^\\pi(s) = {\\sum}_{a\\in A}\\pi(a|s)\\space{\\sum}_{s^{\\prime}\\in S}P(s^{\\prime}|s,a)[R(s,a)+\\gamma V^\\pi(s^{\\prime})]$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $V^\\pi(s):$ Value of function of state $s$ under policy\n",
        "* $P(s^{\\prime}|s,a):$  Transition probability from state $s$ to $s^{\\prime}$ when taking action $a$\n",
        "* $R(s,a):$ Reward obtained after taking action $a$ in state $s$\n",
        "* $\\gamma:$ Discount factor controlling the importance of future rewards\n",
        "* $\\pi(a|s):$ Probability of taking action $a$ in state $s$ under policy"
      ],
      "metadata": {
        "id": "1xDR3RgSxBYs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Bellman Equation for Action Value Function\n",
        "$$Q^\\pi(s,a) = {\\sum}_{s^{\\prime}\\in S}P(s^{\\prime}|s,a)\\cdot[R(s,a)+\\gamma \\sum_{a}\\pi(a^{\\prime}|s^{\\prime})\\cdot Q^\\pi(s^{\\prime},a^{\\prime})]$$\n",
        "\n",
        "Where $Q(s,a)$ represents the expected return for taking action $a$ in state $s$.\n",
        "and following policy afterward."
      ],
      "metadata": {
        "id": "o1cQZBb0w7Rl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q-Learning\n",
        "\n",
        "$$Q^*(s,a) = {\\sum}_{s}P(s^{\\prime}|s,a)\\cdot[R(s,a)+\\gamma \\max_{a}\\pi(a^{\\prime}|s^{\\prime})\\cdot Q^*(s^{\\prime},a^{\\prime})]$$\n",
        "\n"
      ],
      "metadata": {
        "id": "osv0pIem3ob2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `Q-learning` and `SARSA (State-Action-Reward-State-Action)` are reinforcement learning algorithms designed to estimate the optimal policy in a `Markov Decision Process (MDP)`.\n",
        "\n",
        "Both of these algorithms are value-based methods that learn action-value functions, but they differ in how they update their Q-values and handle **exploration** and **exploitation**."
      ],
      "metadata": {
        "id": "Xwf0pjqsfv7v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### `Q-learning`\n",
        "\n",
        "* `Q-learning` is an off-policy algorithm, which means that it learns the optimal policy independently of the agent’s actions. In other words, Q-learning updates its Q-values based on the maximum possible reward from the next state, regardless of the action taken by the agent.\n",
        "* In `Q-learning`, the update is based on the maximum Q-value of the next state, which is the highest possible reward the agent could achieve from the next state, independent of the action actually chosen.\n",
        "* `Q-learning` emphasizes exploration by learning based on the maximum possible future reward, which may result in an overestimation of Q-values as it assumes optimal actions will always be taken.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UMiOb2owgrIy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### `SARSA`\n",
        "* `SARSA` is an on-policy algorithm, which means it updates its Q-values based on the actual actions taken by the agent. It learns from the current policy being followed, which means the agent’s actions in the next state directly influence the Q-value updates.\n",
        "* In `SARSA`, the Q-value is updated based on the action actually chosen in the next state, meaning the update depends on both the next state and the action taken according to the current policy.\n",
        "* `SARSA` is more conservative and aligns its learning with the agent’s current policy. It’s more cautious and avoids the risk of overestimation, as it learns from the actual actions it takes."
      ],
      "metadata": {
        "id": "Ts3CQZTIgS4u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary Table: Differences between Q-learning and SARSA\n",
        "\n",
        "|Aspect\t|Q-learning\t|SARSA|\n",
        "|-------|-----------|-----|\n",
        "|**Policy Type**\t|Off-policy\t|On-policy|\n",
        "|**Update Rule** |Uses the maximum Q-value from the next state\t|Uses the Q-value of the actual next action taken|\n",
        "|**Exploration**\t|Encourages exploration by considering the best future action\t|Learns from the agent’s actual policy and actions|\n",
        "|**Convergence Speed**|\tGenerally faster, as it assumes optimal actions\t|Slower, as it learns based on the actual actions taken|\n",
        "|**Stability**\t|May lead to overestimation of Q-values\t|More stable, less prone to overestimation|\n",
        "|**Suitability**\t|Suitable for environments where the optimal policy is the focus and aggressive exploration is acceptable\t|Suitable for environments where stability and alignment with the current policy are more important|\n",
        "|**Risk of Suboptimal Policy**|\tLower risk, as it always aims for the best possible action|\tHigher risk, as it learns based on the current policy, which may be suboptimal"
      ],
      "metadata": {
        "id": "DvWaBvV0iFas"
      }
    }
  ]
}