{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Reinforcement Learning"
      ],
      "metadata": {
        "id": "3BFpkw2BpZn1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$s_t$ = state at time $t$\n",
        "\n",
        "$a_t$ = action taken at time $t$\n",
        "\n",
        "$r_{t+1}$ = reward after action is taken at time $t$\n",
        "\n",
        "$G_t$ = total return\n",
        "\n",
        "\n",
        "$$G_t = r_{t+1}+r_{t+2}+r_{t+3}+\\cdot\\cdot\\cdot = r_{t+1} + G_{t+1} = r_{t+1} + \\gamma Q(s_{t+1},a_{t+1})$$\n"
      ],
      "metadata": {
        "id": "hCkDICr6pn3J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Subtracting  $Q(s_t,a_t)$ from both sides gives\n",
        "\n",
        "$$G_t-Q(s_t,a_t) = r_{t+1} + \\gamma Q(s_{t+1},a_{t+1})-Q(s_t,a_t)$$\n",
        "\n",
        "$$G_t = Q(s_t,a_t)+\\alpha\\left[r(s_t,a_t)+\\gamma Q(s_{t+1},a_{t+1})-Q(s_t,a_t)\\right]$$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "81oieGWvkYEq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "where\n",
        "$$\\alpha\\left[r(s_t,a_t)+\\gamma Q(s_{t+1},a_{t+1})-Q(s_t,a_t)\\right]$$\n",
        "\n",
        "is the gradient and $\\alpha$ (learning rate) determines how much the gradient will impact the return."
      ],
      "metadata": {
        "id": "D4sRdfk7kz0C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "and $$Q_{new}(s_t,a_t)= Q(s_t,a_t)+\\alpha\\left[r(s_t,a_t)+\\gamma Q(s_{t+1},a_{t+1})-Q(s_t,a_t)\\right]$$"
      ],
      "metadata": {
        "id": "MY8j0I6Dh88H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Bellman Equation for State Value Function**\n",
        "\n",
        "$$V^\\pi(s) = {\\sum}_{a\\in A}\\pi(a|s)\\space{\\sum}_{s^{\\prime}\\in S}P(s^{\\prime}|s,a)\\left[R(s,a)+\\gamma V^\\pi(s^{\\prime})\\right]$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $V^\\pi(s):$ Value of function of state $s$ under policy\n",
        "* $P(s^{\\prime}|s,a):$  Transition probability from state $s$ to $s^{\\prime}$ when taking action $a$\n",
        "* $R(s,a):$ Reward obtained after taking action $a$ in state $s$\n",
        "* $\\gamma:$ Discount factor controlling the importance of future rewards\n",
        "* $\\pi(a|s):$ Probability of taking action $a$ in state $s$ under policy"
      ],
      "metadata": {
        "id": "1xDR3RgSxBYs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This can also be written as:\n",
        "\n",
        "$$V^*(s) = \\max_a{\\sum}_{s^{\\prime}}T(s,a,s^{\\prime})\\left[R(s,a,s^{\\prime})+\\gamma V^*(s^{\\prime})\\right]$$\n",
        "\n",
        "* $T(s, a, s^{\\prime})$ is the **transition probability** from state $s$ to state $s^{\\prime}$, given that the agent chose action $a$.\n",
        "* $R(s, a, s^{\\prime})$ is the **reward** that the agent gets when it goes from state $s$ to state $s^{\\prime}$, given that the agent chose action $a$.\n",
        "* $\\gamma$ is the discount factor\n",
        "\n",
        "After enough time, these estimates converge to:\n",
        "\n",
        "$$V_{k+1}(s) = \\max_a{\\sum}_{s^{\\prime}}T(s,a,s^{\\prime})\\left[R(s,a,s^{\\prime})+\\gamma V_k(s^{\\prime})\\right]$$\n",
        "\n",
        "In this equation, $V_k(s)$ is the estimated value of state $s$ at the $kth$ iteration of the algorithm.\n",
        "\n",
        ".\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fI3gL0EkIeLS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Bellman Equation for Action Value Function\n",
        "$$Q^\\pi(s,a) = {\\sum}_{s^{\\prime}\\in S}P(s^{\\prime}|s,a)\\cdot\\left[R(s,a)+\\gamma \\sum_{a}\\pi(a^{\\prime}|s^{\\prime})\\cdot Q^\\pi(s^{\\prime},a^{\\prime})\\right]$$\n",
        "\n",
        "Where $Q(s,a)$ represents the expected return for taking action $a$ in state $s$.\n",
        "and following policy afterward."
      ],
      "metadata": {
        "id": "o1cQZBb0w7Rl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Q-Learning\n",
        "\n",
        "$$Q^*(s,a) = {\\sum}_{s}P(s^{\\prime}|s,a)\\cdot\\left[R(s,a)+\\gamma \\max_{a}\\pi(a^{\\prime}|s^{\\prime})\\cdot Q^*(s^{\\prime},a^{\\prime})\\right]$$\n",
        "\n",
        "which can be also written as:\n",
        "\n",
        "$$Q_{k+1}(s,a) = {\\sum}_{s}T(s,a, s^{\\prime})\\cdot\\left[R(s,a,s^{\\prime})+\\gamma \\max_{a^{\\prime}}\\pi(a^{\\prime}|s^{\\prime})\\cdot Q_k(s^{\\prime},a^{\\prime})\\right]$$\n",
        "\n",
        "Once we have the optimal Q-Values, defining the optimal policy, noted $π^*(s)$, is trivial: when the agent is in state $s$, it should choose the action with the highest Q-Value for that state.\n"
      ],
      "metadata": {
        "id": "osv0pIem3ob2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `Q-learning` and `SARSA (State-Action-Reward-State-Action)` are reinforcement learning algorithms designed to estimate the optimal policy in a `Markov Decision Process (MDP)`.\n",
        "\n",
        "Both of these algorithms are value-based methods that learn action-value functions, but they differ in how they update their Q-values and handle **exploration** and **exploitation**."
      ],
      "metadata": {
        "id": "Xwf0pjqsfv7v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### `Q-learning`\n",
        "\n",
        "$$Q(s,a)\\leftarrow_{\\alpha}r+\\gamma\\cdot\\max_{a^{\\prime}}Q(s^{\\prime},a^{\\prime})$$\n",
        "\n",
        "* `Q-learning` is an off-policy algorithm, which means that it learns the optimal policy independently of the agent’s actions. In other words, Q-learning updates its Q-values based on the maximum possible reward from the next state, regardless of the action taken by the agent.\n",
        "* In `Q-learning`, the update is based on the maximum Q-value of the next state, which is the highest possible reward the agent could achieve from the next state, independent of the action actually chosen.\n",
        "* `Q-learning` emphasizes exploration by learning based on the maximum possible future reward, which may result in an overestimation of Q-values as it assumes optimal actions will always be taken.\n",
        "* Similarly, the `Q-Learning` algorithm is an adaptation of the Q-Value Iteration algorithm to the situation where the transition probabilities and the rewards are initially unknown. `Q-Learning` works by watching an agent play (e.g., randomly) and gradually improving its estimates of the Q-Values. Once it has accurate Q-Value estimates (or close enough), then the optimal policy is choosing the action that has the highest Q-Value (i.e., the **greedy policy**).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UMiOb2owgrIy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### `SARSA`\n",
        "* `SARSA` is an on-policy algorithm, which means it updates its Q-values based on the actual actions taken by the agent. It learns from the current policy being followed, which means the agent’s actions in the next state directly influence the Q-value updates.\n",
        "* In `SARSA`, the Q-value is updated based on the action actually chosen in the next state, meaning the update depends on both the next state and the action taken according to the current policy.\n",
        "* `SARSA` is more conservative and aligns its learning with the agent’s current policy. It’s more cautious and avoids the risk of overestimation, as it learns from the actual actions it takes."
      ],
      "metadata": {
        "id": "Ts3CQZTIgS4u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary Table: Differences between Q-learning and SARSA\n",
        "\n",
        "|Aspect\t|Q-learning\t|SARSA|\n",
        "|-------|-----------|-----|\n",
        "|**Policy Type**\t|Off-policy\t|On-policy|\n",
        "|**Update Rule** |Uses the maximum Q-value from the next state\t|Uses the Q-value of the actual next action taken|\n",
        "|**Exploration**\t|Encourages exploration by considering the best future action\t|Learns from the agent’s actual policy and actions|\n",
        "|**Convergence Speed**|\tGenerally faster, as it assumes optimal actions\t|Slower, as it learns based on the actual actions taken|\n",
        "|**Stability**\t|May lead to overestimation of Q-values\t|More stable, less prone to overestimation|\n",
        "|**Suitability**\t|Suitable for environments where the optimal policy is the focus and aggressive exploration is acceptable\t|Suitable for environments where stability and alignment with the current policy are more important|\n",
        "|**Risk of Suboptimal Policy**|\tLower risk, as it always aims for the best possible action|\tHigher risk, as it learns based on the current policy, which may be suboptimal"
      ],
      "metadata": {
        "id": "DvWaBvV0iFas"
      }
    }
  ]
}